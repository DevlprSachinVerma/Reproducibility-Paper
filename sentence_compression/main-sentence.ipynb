{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7491969,"sourceType":"datasetVersion","datasetId":4362085},{"sourceId":7575029,"sourceType":"datasetVersion","datasetId":4362019}],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install sacrebleu","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-05T11:57:21.989619Z","iopub.execute_input":"2024-02-05T11:57:21.990257Z","iopub.status.idle":"2024-02-05T11:57:36.452700Z","shell.execute_reply.started":"2024-02-05T11:57:21.990224Z","shell.execute_reply":"2024-02-05T11:57:36.451637Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting sacrebleu\n  Downloading sacrebleu-2.4.0-py3-none-any.whl.metadata (57 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.4/57.4 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting portalocker (from sacrebleu)\n  Downloading portalocker-2.8.2-py3-none-any.whl.metadata (8.5 kB)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (2023.12.25)\nRequirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.9.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (1.24.4)\nRequirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.4.6)\nRequirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (5.1.0)\nDownloading sacrebleu-2.4.0-py3-none-any.whl (106 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.3/106.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading portalocker-2.8.2-py3-none-any.whl (17 kB)\nInstalling collected packages: portalocker, sacrebleu\nSuccessfully installed portalocker-2.8.2 sacrebleu-2.4.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import logging\nimport os\nimport pathlib\nimport random\nimport re\nimport sys\nsys.path.append('/kaggle/input/sentence-compression')\nimport click\nimport sacrebleu\n\nimport torch\n\nimport torch.nn as nn\nimport tqdm\n\nimport config\nimport corpora\n\nimport utils\nfrom main_fix import Network as FixNN\nfrom main_sent import Network as ComNN\nfrom sklearn.metrics import classification_report, precision_recall_fscore_support","metadata":{"execution":{"iopub.status.busy":"2024-02-05T11:59:40.077346Z","iopub.execute_input":"2024-02-05T11:59:40.077890Z","iopub.status.idle":"2024-02-05T11:59:40.087421Z","shell.execute_reply.started":"2024-02-05T11:59:40.077841Z","shell.execute_reply":"2024-02-05T11:59:40.086258Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# Get the current working directory\ncwd = os.getcwd()\n\nprint(cwd)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cwd = os.path.dirname('/kaggle/working/')\nlogger = logging.getLogger(\"main\")","metadata":{"execution":{"iopub.status.busy":"2024-02-05T11:59:47.674296Z","iopub.execute_input":"2024-02-05T11:59:47.674657Z","iopub.status.idle":"2024-02-05T11:59:47.679368Z","shell.execute_reply.started":"2024-02-05T11:59:47.674630Z","shell.execute_reply":"2024-02-05T11:59:47.678394Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"class Network(nn.Module):\n    def __init__(\n        self, word2index, embeddings, prior,\n    ):\n        super().__init__()\n        self.logger = logging.getLogger(f\"{__name__}\")\n        self.word2index = word2index\n        self.index2word = {i: k for k, i in word2index.items()}\n        self.fix_gen = FixNN(\n            embedding_type=\"glove\",\n            vocab_size=len(word2index),\n            embedding_dim=config.embedding_dim,\n            \n            embeddings=embeddings,\n            dropout=config.fix_dropout,\n            hidden_dim=config.fix_hidden_dim,\n        )\n        self.com_nn = ComNN(\n            embeddings=embeddings, hidden_size=config.sem_hidden_dim, prior=prior, device=config.DEV\n        )\n\n    def forward(self, x, target, seq_lens):\n        x1 = nn.utils.rnn.pad_sequence(x, batch_first=True)\n        target = nn.utils.rnn.pad_sequence(target, batch_first=True, padding_value=-1)\n\n        fixations = torch.sigmoid(self.fix_gen(x1, seq_lens))\n        # fixations = None\n\n        loss, pred, atts = self.com_nn(x1, target, fixations)\n        return loss, pred, atts, fixations","metadata":{"execution":{"iopub.status.busy":"2024-02-05T11:59:50.700593Z","iopub.execute_input":"2024-02-05T11:59:50.701430Z","iopub.status.idle":"2024-02-05T11:59:50.709872Z","shell.execute_reply.started":"2024-02-05T11:59:50.701398Z","shell.execute_reply":"2024-02-05T11:59:50.708877Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"def load_corpus(corpus_name, splits):\n    if not splits:\n        return\n\n    logger.info(\"loading corpus\")\n    if corpus_name == \"google\":\n        load_fn = corpora.load_google\n\n    corpus = {}\n    langs = []\n\n    if \"train\" in splits:\n        train_pairs, train_lang = load_fn(\"train\", max_len=200)\n        corpus[\"train\"] = train_pairs\n        langs.append(train_lang)\n    if \"val\" in splits:\n        val_pairs, val_lang = load_fn(\"val\")\n        corpus[\"val\"] = val_pairs\n        langs.append(val_lang)\n    if \"test\" in splits:\n        test_pairs, test_lang = load_fn(\"test\")\n        corpus[\"test\"] = test_pairs\n        langs.append(test_lang)\n\n    logger.info(\"creating word index\")\n    lang = langs[0]\n    for _lang in langs[1:]:\n        lang += _lang\n    word2index = lang.word2index\n\n    index2word = {i: w for w, i in word2index.items()}\n\n    return corpus, word2index, index2word","metadata":{"execution":{"iopub.status.busy":"2024-02-05T11:59:54.953591Z","iopub.execute_input":"2024-02-05T11:59:54.954314Z","iopub.status.idle":"2024-02-05T11:59:54.962162Z","shell.execute_reply.started":"2024-02-05T11:59:54.954281Z","shell.execute_reply":"2024-02-05T11:59:54.961280Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"def init_network(word2index, prior):\n    logger.info(\"loading embeddings\")\n    vocabulary = sorted(word2index.keys())\n    embeddings = utils.load_glove(vocabulary)\n\n    logger.info(\"initializing model\")\n    network = Network(word2index=word2index, embeddings=embeddings, prior=prior)\n    network.to(config.DEV)\n\n    print(f\"#parameters: {sum(p.numel() for p in network.parameters())}\")\n\n    return network","metadata":{"execution":{"iopub.status.busy":"2024-02-05T11:59:57.514047Z","iopub.execute_input":"2024-02-05T11:59:57.514758Z","iopub.status.idle":"2024-02-05T11:59:57.520441Z","shell.execute_reply.started":"2024-02-05T11:59:57.514724Z","shell.execute_reply":"2024-02-05T11:59:57.519466Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"@click.group(context_settings=dict(help_option_names=[\"-h\", \"--help\"]))\n@click.option(\"-v\", \"--verbose\", count=True)\n@click.option(\"-d\", \"--debug\", is_flag=True)\ndef main(verbose, debug):\n    if verbose == 0:\n        loglevel = logging.ERROR\n    elif verbose == 1:\n        loglevel = logging.WARN\n    elif verbose >= 2:\n        loglevel = logging.INFO\n\n    if debug:\n        loglevel = logging.DEBUG\n\n    logging.basicConfig(\n        format=\"[%(asctime)s] <%(name)s> %(levelname)s: %(message)s\",\n        datefmt=\"%d.%m. %H:%M:%S\",\n        level=loglevel,\n    )\n\n    logger.debug(\"arguments: %s\" % str(sys.argv))","metadata":{"execution":{"iopub.status.busy":"2024-02-05T12:00:01.094568Z","iopub.execute_input":"2024-02-05T12:00:01.094925Z","iopub.status.idle":"2024-02-05T12:00:01.101925Z","shell.execute_reply.started":"2024-02-05T12:00:01.094895Z","shell.execute_reply":"2024-02-05T12:00:01.100962Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"def train(corpus_name, model_name, fixation_weights=None, freeze_fixations=False, debug=False, prior=0.5):\n    corpus, word2index, index2word = load_corpus(corpus_name, [\"train\", \"val\"])\n    train_pairs = corpus[\"train\"]\n    val_pairs = corpus[\"val\"]\n    network = init_network(word2index, prior)\n\n    model_dir = os.path.join(\"models\", model_name)\n    logger.debug(\"creating model dir %s\" % model_dir)\n    pathlib.Path(model_dir).mkdir(parents=True)\n\n    if fixation_weights is not None:\n        logger.info(\"loading fixation prediction checkpoint\")\n        checkpoint = torch.load(fixation_weights, map_location=config.DEV)\n        if \"word2index\" in checkpoint:\n            weights = checkpoint[\"weights\"]\n        else:\n            weights = checkpoint\n\n        # remove the embedding layer before loading\n        weights = {\n            k: v for k, v in weights.items() if not k.startswith(\"pre.embedding_layer\")\n        }\n        network.fix_gen.load_state_dict(weights, strict=False)\n\n        if freeze_fixations:\n            logger.info(\"freezing fixation generation network\")\n            for p in network.fix_gen.parameters():\n                p.requires_grad = False\n\n    optimizer = torch.optim.Adam(network.parameters(), lr=config.learning_rate)\n\n\n    epoch = 3\n    # Load previously saved model and word2index\n    checkpoint_path = '/kaggle/input/sentence-compression/sent_3 (1).tar'\n    checkpoint = torch.load(checkpoint_path, map_location=config.DEV)\n    network.load_state_dict(checkpoint[\"weights\"])\n\n    word2index = checkpoint[\"word2index\"]\n    best_val_loss = 2.5685\n    batch_size = 5\n\n    while epoch<6:\n        train_batch_iter = utils.sent_iter(\n            sents=train_pairs, word2index=word2index, batch_size=batch_size\n        )\n        val_batch_iter = utils.sent_iter(\n            sents=val_pairs, word2index=word2index, batch_size=batch_size\n        )\n\n        total_train_loss = 0\n        total_val_loss = 0\n\n        network.train()\n        for i, batch in tqdm.tqdm(\n            enumerate(train_batch_iter, 1), total=len(train_pairs) // batch_size + 1\n        ):\n            optimizer.zero_grad()\n\n            raw_sent, sent, target = batch\n            seq_lens = [len(x) for x in sent]\n            loss, prediction, attention, fixations = network(sent, target, seq_lens)\n\n            prediction = prediction.detach().cpu().numpy()\n\n            torch.nn.utils.clip_grad_norm(network.parameters(), max_norm=5)\n\n            loss.backward()\n            optimizer.step()\n\n            total_train_loss += loss.item()\n\n        avg_train_loss = total_train_loss / len(train_pairs)\n\n        val_sents = []\n        val_preds = []\n        val_targets = []\n\n        network.eval()\n        for i, batch in tqdm.tqdm(\n            enumerate(val_batch_iter), total=len(val_pairs) // batch_size + 1\n        ):\n            raw_sent, sent, target = batch\n            seq_lens = [len(x) for x in sent]\n            loss, prediction, attention, fixations = network(sent, target, seq_lens)\n\n            prediction = prediction.detach().cpu().numpy()\n\n            for i, l in enumerate(seq_lens):\n                val_sents.append(raw_sent[i][:l])\n                val_preds.append(prediction[i][:l].tolist())\n                val_targets.append(target[i][:l].tolist())\n\n            total_val_loss += loss.item()\n\n        avg_val_loss = total_val_loss / len(val_pairs)\n        \n        # Calculate compression ratio\n        compression_ratios = [len(sent) / pred.count(1) if 1 in pred else 0 for sent, pred in zip(val_sents, val_preds)]\n\n\n        print(\n            f\"epoch {epoch} train_loss {avg_train_loss:.4f} val_loss {avg_val_loss:.4f}\"\n        )\n\n        print(\n            classification_report(\n                [x for y in val_targets for x in y],\n                [x for y in val_preds for x in y],\n                target_names=[\"not_del\", \"del\"],\n                digits=5,\n            )\n        )\n        \n        # Print the classification report and compression ratio\n        print(f\"Avg Compression Ratio: {sum(compression_ratios) / len(compression_ratios):.4f}\")\n\n        with open(f\"models/{model_name}/val_original_{epoch}.txt\", \"w\") as oh, open(\n            f\"models/{model_name}/val_pred_{epoch}.txt\", \"w\"\n        ) as ph, open(f\"models/{model_name}/val_gold_{epoch}.txt\", \"w\") as gh:\n            for sent, preds, golds in zip(val_sents, val_preds, val_targets):\n                pred_compressed = [\n                    word for word, delete in zip(sent, preds) if not delete\n                ]\n                gold_compressed = [\n                    word for word, delete in zip(sent, golds) if not delete\n                ]\n\n                oh.write(\" \".join(sent))\n                ph.write(\" \".join(pred_compressed))\n                gh.write(\" \".join(gold_compressed))\n\n                oh.write(\"\\n\")\n                ph.write(\"\\n\")\n                gh.write(\"\\n\")\n\n        if best_val_loss is None or avg_val_loss < best_val_loss:\n            delta = avg_val_loss - best_val_loss if best_val_loss is not None else 0.0\n            best_val_loss = avg_val_loss\n            print(\n                f\"new best model epoch {epoch} val loss {avg_val_loss:.4f} ({delta:.4f})\"\n            )\n\n        utils.save_model(\n            network, word2index, f\"models/{model_name}/{model_name}_{epoch}\"\n        )\n\n        epoch += 1\n","metadata":{"execution":{"iopub.status.busy":"2024-02-05T12:00:03.945043Z","iopub.execute_input":"2024-02-05T12:00:03.945385Z","iopub.status.idle":"2024-02-05T12:00:03.970730Z","shell.execute_reply.started":"2024-02-05T12:00:03.945359Z","shell.execute_reply":"2024-02-05T12:00:03.969522Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"@main.command()\n@click.option(\n    \"-c\",\n    \"--corpus\",\n    \n    \"corpus_name\",\n    required=True,\n    type=click.Choice(sorted([\"google\",])),\n)\n@click.option(\"-w\", \"--model_weights\", required=True)\n@click.option(\"-p\", \"--prior\", type=float, default=.5)\n@click.option(\"-l\", \"--longest\", is_flag=True)\n@click.option(\"-s\", \"--shortest\", is_flag=True)\n@click.option(\"-d\", \"--detailed\", is_flag=True)\ndef test(corpus_name, model_weights, prior, longest, shortest, detailed):\n    if longest and shortest:\n        print(\"longest and shortest are mutually exclusive\", file=sys.stderr)\n        sys.exit()\n\n    corpus, word2index, index2word = load_corpus(corpus_name, [\"test\"])\n    test_pairs = corpus[\"test\"]\n\n    model_name = os.path.basename(os.path.dirname(model_weights))\n    epoch = re.search(\"_(\\d+).tar\", model_weights).group(1)\n\n    logger.info(\"loading model checkpoint\")\n    checkpoint = torch.load(model_weights, map_location=config.DEV)\n    if \"word2index\" in checkpoint:\n        weights = checkpoint[\"weights\"]\n        word2index = checkpoint[\"word2index\"]\n        index2word = {i: w for w, i in word2index.items()}\n    else:\n        asdf\n\n    network = init_network(word2index, prior)\n    network.eval()\n\n    # remove the embedding layer before loading\n    # weights = {k: v for k, v in weights.items() if not \"embedding\" in k}\n    # actually load the parameters\n    network.load_state_dict(weights, strict=False)\n\n    total_test_loss = 0\n\n    batch_size = 20\n\n    test_batch_iter = utils.sent_iter(\n        sents=test_pairs, word2index=word2index, batch_size=batch_size\n    )\n\n    test_sents = []\n    test_preds = []\n    test_targets = []\n\n    for i, batch in tqdm.tqdm(\n        enumerate(test_batch_iter, 1), total=len(test_pairs) // batch_size + 1\n    ):\n        raw_sent, sent, target = batch\n        seq_lens = [len(x) for x in sent]\n        loss, prediction, attention, fixations = network(sent, target, seq_lens)\n\n        prediction = prediction.detach().cpu().numpy()\n\n        for i, l in enumerate(\n            seq_lens\n        ):\n            test_sents.append(raw_sent[i][:l])\n            test_preds.append(prediction[i][:l].tolist())\n            test_targets.append(target[i][:l].tolist())\n\n        total_test_loss += loss.item()\n\n    avg_test_loss = total_test_loss / len(test_pairs)\n\n    print(f\"test_loss {avg_test_loss:.4f}\")\n\n    if longest:\n        avg_len = sum(len(s) for s in test_sents)/len(test_sents)\n        test_sents = list(filter(lambda x: len(x) > avg_len, test_sents))\n        test_preds = list(filter(lambda x: len(x) > avg_len, test_preds))\n        test_targets = list(filter(lambda x: len(x) > avg_len, test_targets))\n    elif shortest:\n        avg_len = sum(len(s) for s in test_sents)/len(test_sents)\n        test_sents = list(filter(lambda x: len(x) <= avg_len, test_sents))\n        test_preds = list(filter(lambda x: len(x) <= avg_len, test_preds))\n        test_targets = list(filter(lambda x: len(x) <= avg_len, test_targets))\n\n    if detailed:\n        for test_sent, test_target, test_pred in zip(test_sents, test_targets, test_preds):\n            print(precision_recall_fscore_support(test_target, test_pred, average=\"weighted\")[2], test_sent, test_target, test_pred)\n    else:\n        print(\n            classification_report(\n                [x for y in test_targets for x in y],\n                [x for y in test_preds for x in y],\n                target_names=[\"not_del\", \"del\"],\n                digits=5,\n            )\n        )\n\n    with open(f\"models/{model_name}/test_original_{epoch}.txt\", \"w\") as oh, open(\n        f\"models/{model_name}/test_pred_{epoch}.txt\", \"w\"\n    ) as ph, open(f\"models/{model_name}/test_gold_{epoch}.txt\", \"w\") as gh:\n        for sent, preds, golds in zip(test_sents, test_preds, test_targets):\n            pred_compressed = [word for word, delete in zip(sent, preds) if not delete]\n            gold_compressed = [word for word, delete in zip(sent, golds) if not delete]\n\n            oh.write(\" \".join(sent))\n            ph.write(\" \".join(pred_compressed))\n            gh.write(\" \".join(gold_compressed))\n\n            oh.write(\"\\n\")\n            ph.write(\"\\n\")\n            gh.write(\"\\n\")","metadata":{"execution":{"iopub.status.busy":"2024-02-05T12:00:06.186120Z","iopub.execute_input":"2024-02-05T12:00:06.186561Z","iopub.status.idle":"2024-02-05T12:00:06.210777Z","shell.execute_reply.started":"2024-02-05T12:00:06.186528Z","shell.execute_reply":"2024-02-05T12:00:06.209870Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"def predict(corpus_name, model_weights, prior=0.5, longest=True, shortest=False):\n    if longest and shortest:\n        print(\"longest and shortest are mutually exclusive\", file=sys.stderr)\n        sys.exit()\n\n    corpus, word2index, index2word = load_corpus(corpus_name, [\"test\"])\n    test_pairs = corpus[\"test\"]\n\n    model_name = os.path.basename(os.path.dirname(model_weights))\n    epoch = 8\n\n    logger.info(\"loading model checkpoint\")\n    checkpoint = torch.load(model_weights, map_location=config.DEV)\n    if \"word2index\" in checkpoint:\n        weights = checkpoint[\"weights\"]\n        word2index = checkpoint[\"word2index\"]\n        index2word = {i: w for w, i in word2index.items()}\n    else:\n        asdf\n\n    network = init_network(word2index, prior)\n    network.eval()\n\n    # remove the embedding layer before loading\n    # weights = {k: v for k, v in weights.items() if not \"embedding\" in k}\n    # actually load the parameters\n    network.load_state_dict(weights, strict=False)\n\n    total_test_loss = 0\n\n    batch_size = 20\n\n    test_batch_iter = utils.sent_iter(\n        sents=test_pairs, word2index=word2index, batch_size=batch_size\n    )\n\n    test_sents = []\n    test_preds = []\n    test_attentions = []\n    test_fixations = []\n\n    for i, batch in tqdm.tqdm(\n        enumerate(test_batch_iter, 1), total=len(test_pairs) // batch_size + 1\n    ):\n        raw_sent, sent, target = batch\n        seq_lens = [len(x) for x in sent]\n        loss, prediction, attention, fixations = network(sent, target, seq_lens)\n\n        prediction = prediction.detach().cpu().numpy()\n        attention = attention.detach().cpu().numpy()\n        if fixations is not None:\n            fixations = fixations.detach().cpu().numpy()\n\n        for i, l in enumerate(\n            seq_lens\n        ):\n            test_sents.append(raw_sent[i][:l])\n            test_preds.append(prediction[i][:l].tolist())\n            test_attentions.append(attention[i][:l].tolist())\n            if fixations is not None:\n                test_fixations.append(fixations[i][:l].tolist())\n            else:\n                test_fixations.append([])\n\n        total_test_loss += loss.item()\n\n    avg_test_loss = total_test_loss / len(test_pairs)\n\n    if longest:\n        avg_len = sum(len(s) for s in test_sents)/len(test_sents)\n        test_sents = list(filter(lambda x: len(x) > avg_len, test_sents))\n        test_preds = list(filter(lambda x: len(x) > avg_len, test_preds))\n        test_attentions = list(filter(lambda x: len(x) > avg_len, test_attentions))\n        test_fixations = list(filter(lambda x: len(x) > avg_len, test_fixations))\n    elif shortest:\n        avg_len = sum(len(s) for s in test_sents)/len(test_sents)\n        test_sents = list(filter(lambda x: len(x) <= avg_len, test_sents))\n        test_preds = list(filter(lambda x: len(x) <= avg_len, test_preds))\n        test_attentions = list(filter(lambda x: len(x) <= avg_len, test_attentions))\n        test_fixations = list(filter(lambda x: len(x) <= avg_len, test_fixations))\n\n    print(f\"sentence\\tprediction\\tattentions\\tfixations\")\n    for s, p, a, f in zip(test_sents, test_preds, test_attentions, test_fixations):\n        a = [x[:len(a)] for x in a]\n        print(f\"{s}\\t{p}\\t{a}\\t{f}\")","metadata":{"execution":{"iopub.status.busy":"2024-02-05T12:00:09.222262Z","iopub.execute_input":"2024-02-05T12:00:09.222628Z","iopub.status.idle":"2024-02-05T12:00:09.243407Z","shell.execute_reply.started":"2024-02-05T12:00:09.222601Z","shell.execute_reply":"2024-02-05T12:00:09.242564Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"train('google','sent')","metadata":{"execution":{"iopub.status.busy":"2024-01-29T13:50:06.003919Z","iopub.execute_input":"2024-01-29T13:50:06.004281Z","iopub.status.idle":"2024-01-29T16:34:49.511853Z","shell.execute_reply.started":"2024-01-29T13:50:06.004251Z","shell.execute_reply":"2024-01-29T16:34:49.510980Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"128 4\n#parameters: 147002715\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/8897 [00:00<?, ?it/s]/tmp/ipykernel_26/533663501.py:60: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n  torch.nn.utils.clip_grad_norm(network.parameters(), max_norm=5)\n100%|██████████| 8897/8897 [50:58<00:00,  2.91it/s]  \n100%|█████████▉| 1000/1001 [02:08<00:00,  7.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch 1 train_loss 1.7352 val_loss 1.5888\n              precision    recall  f1-score   support\n\n     not_del    0.82951   0.89309   0.86013    173095\n         del    0.76864   0.65928   0.70977     93250\n\n    accuracy                        0.81123    266345\n   macro avg    0.79908   0.77619   0.78495    266345\nweighted avg    0.80820   0.81123   0.80749    266345\n\nnew best model epoch 1 val loss 1.5888 (0.0000)\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/8897 [00:00<?, ?it/s]/tmp/ipykernel_26/533663501.py:60: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n  torch.nn.utils.clip_grad_norm(network.parameters(), max_norm=5)\n100%|██████████| 8897/8897 [50:57<00:00,  2.91it/s]  \n100%|█████████▉| 1000/1001 [02:07<00:00,  7.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch 2 train_loss 1.5240 val_loss 1.4919\n              precision    recall  f1-score   support\n\n     not_del    0.84243   0.90543   0.87280    173095\n         del    0.79616   0.68564   0.73678     93250\n\n    accuracy                        0.82848    266345\n   macro avg    0.81930   0.79554   0.80479    266345\nweighted avg    0.82623   0.82848   0.82518    266345\n\nnew best model epoch 2 val loss 1.4919 (-0.0969)\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/8897 [00:00<?, ?it/s]/tmp/ipykernel_26/533663501.py:60: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n  torch.nn.utils.clip_grad_norm(network.parameters(), max_norm=5)\n100%|██████████| 8897/8897 [50:51<00:00,  2.92it/s]  \n100%|█████████▉| 1000/1001 [02:06<00:00,  7.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch 3 train_loss 1.4095 val_loss 1.4265\n              precision    recall  f1-score   support\n\n     not_del    0.85407   0.90413   0.87839    173095\n         del    0.80032   0.71324   0.75428     93250\n\n    accuracy                        0.83730    266345\n   macro avg    0.82720   0.80869   0.81633    266345\nweighted avg    0.83525   0.83730   0.83494    266345\n\nnew best model epoch 3 val loss 1.4265 (-0.0654)\n","output_type":"stream"}]},{"cell_type":"code","source":"train('google','sent')","metadata":{"execution":{"iopub.status.busy":"2024-01-31T16:58:00.910654Z","iopub.execute_input":"2024-01-31T16:58:00.911203Z","iopub.status.idle":"2024-01-31T21:28:21.144167Z","shell.execute_reply.started":"2024-01-31T16:58:00.911158Z","shell.execute_reply":"2024-01-31T21:28:21.142591Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"128 4\n#parameters: 147002715\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/17794 [00:00<?, ?it/s]/tmp/ipykernel_35/3004171176.py:66: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n  torch.nn.utils.clip_grad_norm(network.parameters(), max_norm=5)\n100%|█████████▉| 17793/17794 [1:02:48<00:00,  4.72it/s]\n100%|█████████▉| 2000/2001 [02:35<00:00, 12.84it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch 4 train_loss 2.4024 val_loss 2.4594\n              precision    recall  f1-score   support\n\n     not_del    0.86270   0.90822   0.88488    173095\n         del    0.81114   0.73169   0.76937     93250\n\n    accuracy                        0.84642    266345\n   macro avg    0.83692   0.81996   0.82712    266345\nweighted avg    0.84465   0.84642   0.84444    266345\n\nAvg Compression Ratio: 1.0000\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/17794 [00:00<?, ?it/s]/tmp/ipykernel_35/3004171176.py:66: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n  torch.nn.utils.clip_grad_norm(network.parameters(), max_norm=5)\n100%|█████████▉| 17793/17794 [1:02:41<00:00,  4.73it/s]\n100%|█████████▉| 2000/2001 [02:33<00:00, 12.99it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch 5 train_loss 2.2128 val_loss 2.4321\n              precision    recall  f1-score   support\n\n     not_del    0.86254   0.91486   0.88793    173095\n         del    0.82190   0.72936   0.77287     93250\n\n    accuracy                        0.84991    266345\n   macro avg    0.84222   0.82211   0.83040    266345\nweighted avg    0.84831   0.84991   0.84765    266345\n\nAvg Compression Ratio: 1.0000\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/17794 [00:00<?, ?it/s]/tmp/ipykernel_35/3004171176.py:66: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n  torch.nn.utils.clip_grad_norm(network.parameters(), max_norm=5)\n100%|█████████▉| 17793/17794 [1:02:49<00:00,  4.72it/s]\n100%|█████████▉| 2000/2001 [02:41<00:00, 12.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch 6 train_loss 2.0533 val_loss 2.4533\n              precision    recall  f1-score   support\n\n     not_del    0.86768   0.90992   0.88830    173095\n         del    0.81617   0.74242   0.77755     93250\n\n    accuracy                        0.85128    266345\n   macro avg    0.84193   0.82617   0.83292    266345\nweighted avg    0.84965   0.85128   0.84952    266345\n\nAvg Compression Ratio: 1.0000\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/17794 [00:00<?, ?it/s]/tmp/ipykernel_35/3004171176.py:66: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n  torch.nn.utils.clip_grad_norm(network.parameters(), max_norm=5)\n100%|█████████▉| 17793/17794 [1:03:41<00:00,  4.66it/s]\n100%|█████████▉| 2000/2001 [02:42<00:00, 12.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch 7 train_loss 1.8859 val_loss 2.5894\n              precision    recall  f1-score   support\n\n     not_del    0.86638   0.91056   0.88792    173095\n         del    0.81662   0.73931   0.77605     93250\n\n    accuracy                        0.85061    266345\n   macro avg    0.84150   0.82494   0.83198    266345\nweighted avg    0.84896   0.85061   0.84875    266345\n\nAvg Compression Ratio: 1.0000\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/17794 [00:00<?, ?it/s]/tmp/ipykernel_35/3004171176.py:66: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n  torch.nn.utils.clip_grad_norm(network.parameters(), max_norm=5)\n  3%|▎         | 496/17794 [01:46<1:01:38,  4.68it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgoogle\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[9], line 68\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(corpus_name, model_name, fixation_weights, freeze_fixations, debug, prior)\u001b[0m\n\u001b[1;32m     64\u001b[0m prediction \u001b[38;5;241m=\u001b[39m prediction\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     66\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm(network\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m---> 68\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     71\u001b[0m total_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"train('google','sent')","metadata":{"execution":{"iopub.status.busy":"2024-02-01T21:54:23.967111Z","iopub.execute_input":"2024-02-01T21:54:23.967460Z","iopub.status.idle":"2024-02-01T23:04:22.243166Z","shell.execute_reply.started":"2024-02-01T21:54:23.967435Z","shell.execute_reply":"2024-02-01T23:04:22.242272Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"128 4\n#parameters: 147002715\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/17794 [00:00<?, ?it/s]/tmp/ipykernel_34/3957407179.py:66: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n  torch.nn.utils.clip_grad_norm(network.parameters(), max_norm=5)\n100%|█████████▉| 17793/17794 [1:01:47<00:00,  4.80it/s]\n100%|█████████▉| 2000/2001 [02:29<00:00, 13.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch 8 train_loss 1.7026 val_loss 2.7080\n              precision    recall  f1-score   support\n\n     not_del    0.87317   0.90193   0.88732    173095\n         del    0.80611   0.75683   0.78069     93250\n\n    accuracy                        0.85113    266345\n   macro avg    0.83964   0.82938   0.83401    266345\nweighted avg    0.84969   0.85113   0.84999    266345\n\nAvg Compression Ratio: 3.3871\n","output_type":"stream"}]},{"cell_type":"code","source":"train('google','sent')","metadata":{"execution":{"iopub.status.busy":"2024-02-03T11:07:17.949586Z","iopub.execute_input":"2024-02-03T11:07:17.950290Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"128 4\n#parameters: 147002715\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/17794 [00:00<?, ?it/s]/tmp/ipykernel_34/3636654160.py:61: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n  torch.nn.utils.clip_grad_norm(network.parameters(), max_norm=5)\n100%|█████████▉| 17793/17794 [1:30:37<00:00,  3.27it/s]  \n100%|█████████▉| 2000/2001 [03:29<00:00,  9.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch 1 train_loss 3.0409 val_loss 2.7835\n              precision    recall  f1-score   support\n\n     not_del    0.84072   0.89385   0.86647    173095\n         del    0.77677   0.68564   0.72837     93250\n\n    accuracy                        0.82095    266345\n   macro avg    0.80874   0.78975   0.79742    266345\nweighted avg    0.81833   0.82095   0.81812    266345\n\nAvg Compression Ratio: 3.5310\nnew best model epoch 1 val loss 2.7835 (0.0000)\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/17794 [00:00<?, ?it/s]/tmp/ipykernel_34/3636654160.py:61: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n  torch.nn.utils.clip_grad_norm(network.parameters(), max_norm=5)\n100%|█████████▉| 17793/17794 [1:30:45<00:00,  3.27it/s]  \n100%|█████████▉| 2000/2001 [03:30<00:00,  9.52it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch 2 train_loss 2.6296 val_loss 2.5685\n              precision    recall  f1-score   support\n\n     not_del    0.85364   0.90658   0.87931    173095\n         del    0.80402   0.71146   0.75492     93250\n\n    accuracy                        0.83827    266345\n   macro avg    0.82883   0.80902   0.81711    266345\nweighted avg    0.83627   0.83827   0.83576    266345\n\nAvg Compression Ratio: 3.5496\nnew best model epoch 2 val loss 2.5685 (-0.2150)\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/17794 [00:00<?, ?it/s]/tmp/ipykernel_34/3636654160.py:61: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n  torch.nn.utils.clip_grad_norm(network.parameters(), max_norm=5)\n 96%|█████████▌| 17055/17794 [1:26:59<04:00,  3.07it/s]  ","output_type":"stream"}]},{"cell_type":"code","source":"train('google','sent')","metadata":{"execution":{"iopub.status.busy":"2024-02-05T12:00:28.496827Z","iopub.execute_input":"2024-02-05T12:00:28.497257Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"128 4\n#parameters: 147002715\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/17794 [00:00<?, ?it/s]/tmp/ipykernel_34/934650746.py:66: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n  torch.nn.utils.clip_grad_norm(network.parameters(), max_norm=5)\n100%|█████████▉| 17793/17794 [1:32:12<00:00,  3.22it/s]  \n100%|█████████▉| 2000/2001 [03:34<00:00,  9.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch 3 train_loss 2.2247 val_loss 2.4341\n              precision    recall  f1-score   support\n\n     not_del    0.86696   0.90984   0.88788    173095\n         del    0.81573   0.74084   0.77648     93250\n\n    accuracy                        0.85067    266345\n   macro avg    0.84134   0.82534   0.83218    266345\nweighted avg    0.84902   0.85067   0.84888    266345\n\nAvg Compression Ratio: 3.4641\nnew best model epoch 3 val loss 2.4341 (-0.1344)\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/17794 [00:00<?, ?it/s]/tmp/ipykernel_34/934650746.py:66: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n  torch.nn.utils.clip_grad_norm(network.parameters(), max_norm=5)\n 62%|██████▏   | 11019/17794 [57:22<31:05,  3.63it/s]  ","output_type":"stream"}]}]}