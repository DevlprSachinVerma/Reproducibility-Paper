{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7385484,"sourceType":"datasetVersion","datasetId":4292722},{"sourceId":7574895,"sourceType":"datasetVersion","datasetId":4275825}],"dockerImageVersionId":30626,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install sacrebleu","metadata":{"_uuid":"adcb0af6-48be-446b-b093-ccbe11a49aa8","_cell_guid":"0ded8290-611c-43c2-80ad-a0c3c7fc3f31","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-06T21:01:25.366289Z","iopub.execute_input":"2024-02-06T21:01:25.367103Z","iopub.status.idle":"2024-02-06T21:01:39.772233Z","shell.execute_reply.started":"2024-02-06T21:01:25.367058Z","shell.execute_reply":"2024-02-06T21:01:39.770978Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting sacrebleu\n  Obtaining dependency information for sacrebleu from https://files.pythonhosted.org/packages/de/ea/025db0a39337b63d4728a900d262c39c3029b0fe76a9876ce6297b1aa6a0/sacrebleu-2.4.0-py3-none-any.whl.metadata\n  Downloading sacrebleu-2.4.0-py3-none-any.whl.metadata (57 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.4/57.4 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hCollecting portalocker (from sacrebleu)\n  Obtaining dependency information for portalocker from https://files.pythonhosted.org/packages/17/9e/87671efcca80ba6203811540ed1f9c0462c1609d2281d7b7f53cef05da3d/portalocker-2.8.2-py3-none-any.whl.metadata\n  Downloading portalocker-2.8.2-py3-none-any.whl.metadata (8.5 kB)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (2023.8.8)\nRequirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.9.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (1.24.3)\nRequirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.4.6)\nRequirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (4.9.3)\nDownloading sacrebleu-2.4.0-py3-none-any.whl (106 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.3/106.3 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading portalocker-2.8.2-py3-none-any.whl (17 kB)\nInstalling collected packages: portalocker, sacrebleu\nSuccessfully installed portalocker-2.8.2 sacrebleu-2.4.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import logging\nimport os\nimport pathlib\nimport random\nimport sys\nsys.path.append('/kaggle/input/supporting-files')\nimport click\n\nimport sacrebleu\nimport torch\nimport torch.nn as nn\nimport tqdm\nimport config\nimport corpora\nimport utils\nfrom main_fix2 import Network as FixNN\nfrom main_para import EncoderRNN as ParEncNN\nfrom main_para import AttnDecoderRNN as ParDecNN","metadata":{"_uuid":"21f99c5a-287c-4af3-aaf4-4f7c6db7d23a","_cell_guid":"7547be06-7a49-4b71-99c7-c65ad81ed63d","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-06T21:11:17.952817Z","iopub.execute_input":"2024-02-06T21:11:17.953707Z","iopub.status.idle":"2024-02-06T21:11:17.967394Z","shell.execute_reply.started":"2024-02-06T21:11:17.953664Z","shell.execute_reply":"2024-02-06T21:11:17.966594Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"#pip install config","metadata":{"_uuid":"f40ef613-6b36-4311-9642-6ad47162c90e","_cell_guid":"03e581b8-5ac2-40ed-8bae-0b4b79cb4241","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-24T12:20:04.544600Z","iopub.execute_input":"2024-01-24T12:20:04.545408Z","iopub.status.idle":"2024-01-24T12:20:04.549397Z","shell.execute_reply.started":"2024-01-24T12:20:04.545378Z","shell.execute_reply":"2024-01-24T12:20:04.548434Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"\t#qqp sentences:\n\ndebug_sentences = [\n    'How do I get funding for my web based startup idea ?',\n    'What do intelligent people do to pass time ?',\n    'Which is the best SEO Company in Delhi ?',\n    'Why do you waer makeup ?',\n    'How do start chatting with a girl ?',\n    'What is the meaning of living life ?',\n    'Why do my armpits hurt ?',\n    'Why does eye color change with age ?',\n    'How do you find the standard deviation of a probability distribution ? What are some examples ?',\n    'How can I complete my 11 syllabus in one month ?',\n    'How do I concentrate better on my studies ?',\n    'Which is the best retirement plan in india ?',\n    'Should I tell my best friend I love her ?',\n    'Which is the best company for Appian Vagrant online job support ?',\n    'How can one do for good handwriting ?',\n    'What are remedies to get rid of belly fat ?',\n    'What is the best way to cook precooked turkey ?',\n    'What is the future of e-commerce in India ?',\n    'Why do my burps taste like rotten eggs ?',\n    'What is an example of chemical weathering ?',\n    'What are some of the advantages and disadvantages of cyber schooling ?',\n    'How can I increase traffic to my websites by Facebook ?',\n    'How do I increase my patience level in life ?',\n    'What are the best hospitals for treating cancer in India ?',\n    'Will Jio sim work in a 3G phone ? If yes , how ?',\n]\n\ndebug_sentences = [s.split(\" \") for s in debug_sentences]","metadata":{"execution":{"iopub.status.busy":"2024-02-06T21:11:23.298011Z","iopub.execute_input":"2024-02-06T21:11:23.298851Z","iopub.status.idle":"2024-02-06T21:11:23.304657Z","shell.execute_reply.started":"2024-02-06T21:11:23.298811Z","shell.execute_reply":"2024-02-06T21:11:23.303648Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# Get the current working directory\ncwd = os.getcwd()\nprint(cwd)","metadata":{"_uuid":"f878eaa4-869a-4c40-99ab-dd283d96c571","_cell_guid":"d6dfd818-bca9-40ce-89cb-b4f5552bee2c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-06T21:11:26.031788Z","iopub.execute_input":"2024-02-06T21:11:26.032134Z","iopub.status.idle":"2024-02-06T21:11:26.037049Z","shell.execute_reply.started":"2024-02-06T21:11:26.032105Z","shell.execute_reply":"2024-02-06T21:11:26.036256Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"/kaggle/working\n","output_type":"stream"}]},{"cell_type":"code","source":"cwd = os.path.dirname('/kaggle/working/')\nlogger = logging.getLogger(\"main\")","metadata":{"_uuid":"8cd9b574-52de-4cb6-abba-9fa03648072d","_cell_guid":"a5176401-7408-4ac2-b0c1-62c1210c1474","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-06T21:11:28.582696Z","iopub.execute_input":"2024-02-06T21:11:28.583068Z","iopub.status.idle":"2024-02-06T21:11:28.587523Z","shell.execute_reply.started":"2024-02-06T21:11:28.583037Z","shell.execute_reply":"2024-02-06T21:11:28.586603Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"class Network(nn.Module):\n    def __init__(\n        self,\n        word2index,\n        embeddings,\n    ):\n        super().__init__()\n        self.logger = logging.getLogger(f\"{__name__}\")\n        self.word2index = word2index\n        self.index2word = {i: k for k, i in word2index.items()}\n        self.fix_gen = FixNN(\n            embedding_type=\"glove\",\n            vocab_size=len(word2index),\n            embedding_dim=config.embedding_dim,\n            embeddings=embeddings,\n            dropout=config.fix_dropout,\n            hidden_dim=config.fix_hidden_dim,\n        )\n        self.par_enc = ParEncNN(\n            input_size=config.embedding_dim,\n            hidden_size=config.par_hidden_dim,\n            embeddings=embeddings,\n        )\n        self.par_dec = ParDecNN(\n            input_size=config.embedding_dim,\n            hidden_size=config.par_hidden_dim,\n            output_size=len(word2index),\n            embeddings=embeddings,\n            dropout_p=config.par_dropout,\n            max_length=config.max_length,\n        )\n\n    def forward(self, x, target=None, teacher_forcing_ratio=None):\n        teacher_forcing_ratio = teacher_forcing_ratio if teacher_forcing_ratio is not None else config.teacher_forcing_ratio\n        x1 = nn.utils.rnn.pad_sequence(x, batch_first=True)\n        x2 = nn.utils.rnn.pad_sequence(x, batch_first=False)\n        fixations = torch.sigmoid(self.fix_gen(x1, [len(_x) for _x in x1]))\n\n        enc_hidden = self.par_enc.initHidden().to(config.DEV)\n        enc_outs = torch.zeros(config.max_length, config.par_hidden_dim, device=config.DEV)\n\n        for ei in range(len(x2)):\n            enc_out, enc_hidden = self.par_enc(x2[ei], enc_hidden)\n            enc_outs[ei] += enc_out[0, 0]\n\n        dec_in = torch.tensor([[self.word2index[config.SOS]]], device=config.DEV)  # SOS\n        dec_hidden = enc_hidden\n        dec_outs = []\n        dec_words = []\n        dec_atts = torch.zeros(config.max_length, config.max_length)\n\n        if target is not None:  # training\n            target = nn.utils.rnn.pad_sequence(target, batch_first=False)\n            use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n\n            if use_teacher_forcing:\n                for di in range(len(target)):\n                    dec_out, dec_hidden, dec_att = self.par_dec(\n                        dec_in, dec_hidden, enc_outs, fixations\n                    )\n                    dec_outs.append(dec_out)\n                    dec_atts[di] = dec_att.data\n                    dec_input = target[di]\n\n            else:\n                for di in range(len(target)):\n                    dec_out, dec_hidden, dec_att = self.par_dec(\n                        dec_in, dec_hidden, enc_outs, fixations\n                    )\n                    dec_outs.append(dec_out)\n                    dec_atts[di] = dec_att.data\n                    topv, topi = dec_out.data.topk(1)\n                    dec_words.append(self.index2word[topi.item()])\n\n                    dec_input = topi.squeeze().detach()\n\n        else:  # prediction\n            for di in range(config.max_length):\n                dec_out, dec_hidden, dec_att = self.par_dec(\n                    dec_in, dec_hidden, enc_outs, fixations\n                )\n                dec_outs.append(dec_out)\n                dec_atts[di] = dec_att.data\n                topv, topi = dec_out.data.topk(1)\n                if topi.item() == self.word2index[config.EOS]:\n                    dec_words.append(\"<__EOS__>\")\n                    break\n                else:\n                    dec_words.append(self.index2word[topi.item()])\n\n                dec_input = topi.squeeze().detach()\n\n        return dec_outs, dec_words, dec_atts[: di + 1], fixations","metadata":{"_uuid":"ea88f1b9-fbd7-4e04-a54a-8528de09725e","_cell_guid":"e132c2f4-0064-4f7c-b935-06d4d8ec33cf","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-06T21:11:30.726172Z","iopub.execute_input":"2024-02-06T21:11:30.726564Z","iopub.status.idle":"2024-02-06T21:11:30.747691Z","shell.execute_reply.started":"2024-02-06T21:11:30.726531Z","shell.execute_reply":"2024-02-06T21:11:30.746789Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"def load_corpus(corpus_name, splits):\n    if not splits:\n        return\n\n    logger.info(\"loading corpus\")\n    if corpus_name == \"msrpc\":\n        load_fn = corpora.load_msrpc\n    elif corpus_name == \"qqp\":\n        load_fn = corpora.load_qqp\n    elif corpus_name == \"wiki\":\n        load_fn = corpora.load_wiki\n    elif corpus_name == \"qqp_paws\":\n        load_fn = corpora.load_qqp_paws\n    elif corpus_name == \"qqp_kag\":\n        load_fn = corpora.load_qqp_kag\n    elif corpus_name == \"sentiment\":\n        load_fn = corpora.load_sentiment\n    elif corpus_name == \"stanford\":\n        load_fn = corpora.load_stanford\n    elif corpus_name == \"stanford_sent\":\n        load_fn = corpora.load_stanford_sent\n    elif corpus_name == \"tamil\":\n        load_fn = corpora.load_tamil\n    elif corpus_name == \"compression\":\n        load_fn = corpora.load_compression\n\n    corpus = {}\n    langs = []\n\n    if \"train\" in splits:\n        train_pairs, train_lang = load_fn(\"train\")\n        corpus[\"train\"] = train_pairs\n        langs.append(train_lang)\n    if \"val\" in splits:\n        val_pairs, val_lang = load_fn(\"val\")\n        corpus[\"val\"] = val_pairs\n        langs.append(val_lang)\n    if \"test\" in splits:\n        test_pairs, test_lang = load_fn(\"test\")\n        corpus[\"test\"] = test_pairs\n        langs.append(test_lang)\n\n    logger.info(\"creating word index\")\n    lang = langs[0]\n    for _lang in langs[1:]:\n        lang += _lang\n    word2index = lang.word2index\n\n    index2word = {i: w for w, i in word2index.items()}\n\n    return corpus, word2index, index2word\n\n\ndef init_network(word2index):\n    logger.info(\"loading embeddings\")\n    vocabulary = sorted(word2index.keys())\n    embeddings = utils.load_glove(vocabulary)\n\n    logger.info(\"initializing model\")\n    network = Network(\n        word2index=word2index,\n        embeddings=embeddings,\n    )\n    network.to(config.DEV)\n\n    print(f\"#parameters: {sum(p.numel() for p in network.parameters())}\")\n\n    return network","metadata":{"_uuid":"44dcae0e-90ac-4f54-af55-9d6117981437","_cell_guid":"01229423-a75c-4d28-a667-ce61c2576ff3","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-06T21:11:35.468170Z","iopub.execute_input":"2024-02-06T21:11:35.468526Z","iopub.status.idle":"2024-02-06T21:11:35.480396Z","shell.execute_reply.started":"2024-02-06T21:11:35.468494Z","shell.execute_reply":"2024-02-06T21:11:35.479483Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"@click.group(context_settings=dict(help_option_names=[\"-h\", \"--help\"]))\n@click.option(\"-v\", \"--verbose\", count=True)\n@click.option(\"-d\", \"--debug\", is_flag=True)\ndef main(verbose, debug):\n    if verbose == 0:\n        loglevel = logging.ERROR\n    elif verbose == 1:\n        loglevel = logging.WARN\n    elif verbose >= 2:\n        loglevel = logging.INFO\n\n    if debug:\n        loglevel = logging.DEBUG\n\n    logging.basicConfig(\n        format=\"[%(asctime)s] <%(name)s> %(levelname)s: %(message)s\",\n        datefmt=\"%d.%m. %H:%M:%S\",\n        level=loglevel,\n    )\n\n    logger.debug(\"arguments: %s\" % str(sys.argv))","metadata":{"_uuid":"d824795c-2ad7-485d-bfc4-0cf9202e249d","_cell_guid":"a2faabea-e349-4f7d-920d-58fb770e9c97","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-06T21:11:37.585723Z","iopub.execute_input":"2024-02-06T21:11:37.586319Z","iopub.status.idle":"2024-02-06T21:11:37.593115Z","shell.execute_reply.started":"2024-02-06T21:11:37.586285Z","shell.execute_reply":"2024-02-06T21:11:37.592160Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"def train(corpus_name, model_name, bleu, fixation_weights=None, freeze_fixations=False):\n    corpus, word2index, index2word = load_corpus(corpus_name, [\"train\", \"val\"])\n    train_pairs = corpus[\"train\"]\n    val_pairs = corpus[\"val\"]\n    network = init_network(word2index)\n\n    model_dir = os.path.join(\"models\", model_name)\n    logger.debug(\"creating model dir %s\" % model_dir)\n    pathlib.Path(model_dir).mkdir(parents=True)\n\n    if fixation_weights is not None:\n        logger.info(\"loading fixation prediction checkpoint\")\n        checkpoint = torch.load(fixation_weights, map_location=config.DEV)\n        if \"word2index\" in checkpoint:\n            weights = checkpoint[\"weights\"]\n        else:\n            weights = checkpoint\n\n        # remove the embedding layer before loading\n        weights = {k: v for k, v in weights.items() if not k.startswith(\"pre.embedding_layer\")}\n        network.fix_gen.load_state_dict(weights, strict=False)\n\n        if freeze_fixations:\n            logger.info(\"freezing fixation generation network\")\n            for p in network.fix_gen.parameters():\n                p.requires_grad = False\n\n    loss_fn = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(network.parameters(), lr=config.learning_rate)\n    #optimizer = torch.optim.Adam(network.parameters(), lr=1e-4, weight_decay=1e-5)\n\n    best_val_loss = None\n\n    epoch = 1\n    \n    while epoch<6:\n        train_batch_iter = utils.pair_iter(pairs=train_pairs, word2index=word2index, shuffle=True, shuffle_pairs=False)\n        val_batch_iter = utils.pair_iter(pairs=val_pairs, word2index=word2index, shuffle=False, shuffle_pairs=False)\n        # test_batch_iter = utils.pair_iter(pairs=test_pairs, word2index=word2index, shuffle=False, shuffle_pairs=False)\n\n        running_train_loss = 0\n        total_train_loss = 0\n        total_val_loss = 0\n\n        if bleu == \"sacrebleu\":\n            running_train_bleu = 0\n            total_train_bleu = 0\n            total_val_bleu = 0\n        elif bleu == \"nltk\":\n            running_train_bleu_1 = 0\n            running_train_bleu_2 = 0\n            running_train_bleu_3 = 0\n            running_train_bleu_4 = 0\n            total_train_bleu_1 = 0\n            total_train_bleu_2 = 0\n            total_train_bleu_3 = 0\n            total_train_bleu_4 = 0\n            total_val_bleu_1 = 0\n            total_val_bleu_2 = 0\n            total_val_bleu_3 = 0\n            total_val_bleu_4 = 0\n\n        network.train()\n        for i, batch in enumerate(train_batch_iter, 1):\n            optimizer.zero_grad()\n\n            input, target,rating = batch\n            prediction, words, attention, fixations = network(input, target)\n\n            loss = loss_fn(\n                torch.stack(prediction).squeeze(1)[: target[0].shape[0], :], target[0]\n            )\n\n            loss.backward()\n            optimizer.step()\n\n            running_train_loss += loss.item()\n            total_train_loss += loss.item()\n\n            _prediction = \" \".join([index2word[_x] for _x in torch.argmax(torch.stack(prediction).squeeze(1)[: target[0].shape[0], :], -1).tolist()])\n            _target = [index2word[_x] for _x in target[0].tolist()]\n\n            if bleu == \"sacrebleu\":\n                bleu_score = sacrebleu.sentence_bleu(_prediction, _target).score\n                running_train_bleu += bleu_score\n                total_train_bleu += bleu_score\n            elif bleu == \"nltk\":\n                bleu_1_score = utils.bleu(target[0].tolist(), torch.argmax(torch.stack(prediction).squeeze(1)[: target[0].shape[0], :], -1).tolist(), n=1)\n                bleu_2_score = utils.bleu(target[0].tolist(), torch.argmax(torch.stack(prediction).squeeze(1)[: target[0].shape[0], :], -1).tolist(), n=2)\n                bleu_3_score = utils.bleu(target[0].tolist(), torch.argmax(torch.stack(prediction).squeeze(1)[: target[0].shape[0], :], -1).tolist(), n=3)\n                bleu_4_score = utils.bleu(target[0].tolist(), torch.argmax(torch.stack(prediction).squeeze(1)[: target[0].shape[0], :], -1).tolist(), n=4)\n                running_train_bleu_1 += bleu_1_score\n                running_train_bleu_2 += bleu_2_score\n                running_train_bleu_3 += bleu_3_score\n                running_train_bleu_4 += bleu_4_score\n                total_train_bleu_1 += bleu_1_score\n                total_train_bleu_2 += bleu_2_score\n                total_train_bleu_3 += bleu_3_score\n                total_train_bleu_4 += bleu_4_score\n            # print(target[0].tolist(), torch.argmax(torch.stack(prediction).squeeze(1)[: target[0].shape[0], :], -1).tolist())\n\n            if i % 1000 == 0:\n\n                if bleu == \"sacrebleu\":\n\n                    print(f\"step {i} avg_train_loss {running_train_loss/100:.4f} avg_train_bleu {running_train_bleu/100:.2f}\")\n                elif bleu == \"nltk\":\n                    print(f\"step {i} avg_train_loss {running_train_loss/100:.4f} avg_train_bleu_1 {running_train_bleu_1/100:.2f} avg_train_bleu_2 {running_train_bleu_2/100:.2f} avg_train_bleu_3 {running_train_bleu_3/100:.2f} avg_train_bleu_4 {running_train_bleu_4/100:.2f}\")\n\n                network.eval()\n                with open(os.path.join(model_dir, f\"debug_{epoch}_{i}.out\"), \"w\") as h:\n                    if bleu == \"sacrebleu\":\n                        h.write(f\"# avg_train_loss {running_train_loss/100:.4f} avg_train_bleu {running_train_bleu/100:.2f}\")\n                        running_train_bleu = 0\n                    elif bleu == \"nltk\":\n                        h.write(f\"# avg_train_loss {running_train_loss/100:.4f} avg_train_bleu_1 {running_train_bleu_1/100:.2f} avg_train_bleu_2 {running_train_bleu_2/100:.2f} avg_train_bleu_3 {running_train_bleu_3/100:.2f} avg_train_bleu_4 {running_train_bleu_4/100:.2f}\")\n                        running_train_bleu_1 = 0\n                        running_train_bleu_2 = 0\n                        running_train_bleu_3 = 0\n                        running_train_bleu_4 = 0\n\n                    running_train_loss = 0\n\n                    h.write(\"\\n\")\n                    h.write(\"\\t\".join([\"sentence\", \"prediction\", \"attention\", \"fixations\"]))\n                    h.write(\"\\n\")\n                    for s, input in zip(debug_sentences, utils.sent_iter(debug_sentences, word2index=word2index)):\n                        prediction, words, attentions, fixations = network(input)\n                        prediction = torch.argmax(torch.stack(prediction).squeeze(1), -1).detach().cpu().tolist()\n                        prediction = [index2word.get(x, \"<__UNK__>\") for x in prediction]\n                        attentions = attentions.detach().cpu().squeeze().tolist()\n                        fixations = fixations.detach().cpu().squeeze().tolist()\n                        h.write(f\"{s}\\t{prediction}\\t{attentions}\\t{fixations}\")\n                        h.write(\"\\n\")\n\n                network.train()\n\n        network.eval()\n        for i, batch in enumerate(val_batch_iter):\n            input, target ,rating = batch\n            prediction, words, attention, fixations = network(input, target, teacher_forcing_ratio=0)\n            loss = loss_fn(\n                torch.stack(prediction).squeeze(1)[: target[0].shape[0], :], target[0]\n            )\n\n            _prediction = \" \".join([index2word[_x] for _x in torch.argmax(torch.stack(prediction).squeeze(1)[: target[0].shape[0], :], -1).tolist()])\n            _target = [index2word[_x] for _x in target[0].tolist()]\n            if bleu == \"sacrebleu\":\n                bleu_score = sacrebleu.sentence_bleu(_prediction, _target).score\n                total_val_bleu += bleu_score\n            elif bleu == \"nltk\":\n                bleu_1_score = utils.bleu(target[0].tolist(), torch.argmax(torch.stack(prediction).squeeze(1)[: target[0].shape[0], :], -1).tolist(), n=1)\n                bleu_2_score = utils.bleu(target[0].tolist(), torch.argmax(torch.stack(prediction).squeeze(1)[: target[0].shape[0], :], -1).tolist(), n=2)\n                bleu_3_score = utils.bleu(target[0].tolist(), torch.argmax(torch.stack(prediction).squeeze(1)[: target[0].shape[0], :], -1).tolist(), n=3)\n                bleu_4_score = utils.bleu(target[0].tolist(), torch.argmax(torch.stack(prediction).squeeze(1)[: target[0].shape[0], :], -1).tolist(), n=4)\n                total_val_bleu_1 += bleu_1_score\n                total_val_bleu_2 += bleu_2_score\n                total_val_bleu_3 += bleu_3_score\n                total_val_bleu_4 += bleu_4_score\n\n            total_val_loss += loss.item()\n\n        avg_val_loss = total_val_loss/len(val_pairs)\n\n        if bleu == \"sacrebleu\":\n            print(f\"epoch {epoch} avg_train_loss {total_train_loss/len(train_pairs):.4f} avg_val_loss {avg_val_loss:.4f} avg_train_bleu {total_train_bleu/len(train_pairs):.2f} avg_val_bleu {total_val_bleu/len(val_pairs):.2f}\")\n        elif bleu == \"nltk\":\n            print(f\"epoch {epoch} avg_train_loss {total_train_loss/len(train_pairs):.4f} avg_val_loss {avg_val_loss:.4f} avg_train_bleu_1 {total_train_bleu_1/len(train_pairs):.2f} avg_train_bleu_2 {total_train_bleu_2/len(train_pairs):.2f} avg_train_bleu_3 {total_train_bleu_3/len(train_pairs):.2f} avg_train_bleu_4 {total_train_bleu_4/len(train_pairs):.2f} avg_val_bleu_1 {total_val_bleu_1/len(val_pairs):.2f} avg_val_bleu_2 {total_val_bleu_2/len(val_pairs):.2f} avg_val_bleu_3 {total_val_bleu_3/len(val_pairs):.2f} avg_val_bleu_4 {total_val_bleu_4/len(val_pairs):.2f}\")\n\n        with open(os.path.join(model_dir, f\"debug_{epoch}_end.out\"), \"w\") as h:\n            if bleu == \"sacrebleu\":\n                h.write(f\"# avg_train_loss {total_train_loss/len(train_pairs)} avg_val_loss {total_val_loss/len(val_pairs)} avg_train_bleu {total_train_bleu/len(train_pairs)} avg_val_bleu {total_val_bleu/len(val_pairs)}\")\n            elif bleu == \"nltk\":\n                h.write(f\"# avg_train_loss {total_train_loss/len(train_pairs):.4f} avg_val_loss {avg_val_loss:.4f} avg_train_bleu_1 {total_train_bleu_1/len(train_pairs):.2f} avg_train_bleu_2 {total_train_bleu_2/len(train_pairs):.2f} avg_train_bleu_3 {total_train_bleu_3/len(train_pairs):.2f} avg_train_bleu_4 {total_train_bleu_4/len(train_pairs):.2f} avg_val_bleu_1 {total_val_bleu_1/len(val_pairs):.2f} avg_val_bleu_2 {total_val_bleu_2/len(val_pairs):.2f} avg_val_bleu_3 {total_val_bleu_3/len(val_pairs):.2f} avg_val_bleu_4 {total_val_bleu_4/len(val_pairs):.2f}\")\n            h.write(\"\\n\")\n            h.write(\"\\t\".join([\"sentence\", \"prediction\", \"attention\", \"fixations\"]))\n            h.write(\"\\n\")\n            for s, input in zip(debug_sentences, utils.sent_iter(debug_sentences, word2index=word2index)):\n                prediction, words, attentions, fixations = network(input)\n                prediction = torch.argmax(torch.stack(prediction).squeeze(1), -1).detach().cpu().tolist()\n                prediction = [index2word.get(x, \"<__UNK__>\") for x in prediction]\n                attentions = attentions.detach().cpu().squeeze().tolist()\n                fixations = fixations.detach().cpu().squeeze().tolist()\n                h.write(f\"{s}\\t{prediction}\\t{attentions}\\t{fixations}\")\n                h.write(\"\\n\")\n                \n        utils.save_model(network, word2index, os.path.join(model_dir, f\"{model_name}_{epoch}\"))\n\n        if best_val_loss is None or avg_val_loss < best_val_loss:\n            if best_val_loss is not None:\n                logger.info(f\"{avg_val_loss} < {best_val_loss} ({avg_val_loss-best_val_loss}): new best model from epoch {epoch}\")\n            else:\n                logger.info(f\"{avg_val_loss} < {best_val_loss}: new best model from epoch {epoch}\")\n\n            best_val_loss = avg_val_loss\n            # save_model(model, word2index, model_name + \"_epoch_\" + str(epoch))\n            # utils.save_model(network, word2index, os.path.join(model_dir, f\"{model_name}_best\"))\n\n        epoch += 1","metadata":{"_uuid":"46b75b89-1a33-4f12-914f-61d80b77e48d","_cell_guid":"10704351-1f59-4de6-a313-f6defe879a25","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-06T21:11:39.976198Z","iopub.execute_input":"2024-02-06T21:11:39.976524Z","iopub.status.idle":"2024-02-06T21:11:40.020365Z","shell.execute_reply.started":"2024-02-06T21:11:39.976496Z","shell.execute_reply":"2024-02-06T21:11:40.019409Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"def val(corpus_name, model_weights, sentence_statistics, bleu):\n    corpus, word2index, index2word = load_corpus(corpus_name, [\"val\"])\n    val_pairs = corpus[\"val\"]\n\n    logger.info(\"loading model checkpoint\")\n    checkpoint = torch.load(model_weights, map_location=config.DEV)\n    if \"word2index\" in checkpoint:\n        weights = checkpoint[\"weights\"]\n        word2index = checkpoint[\"word2index\"]\n        index2word = {i: w for w, i in word2index.items()}\n    else:\n        asdf\n\n    network = init_network(word2index)\n\n    # remove the embedding layer before loading\n    weights = {k: v for k, v in weights.items() if not \"embedding\" in k}\n    # make a new output layer to match the weights from the checkpoint\n    # we cannot remove it like we did with the embedding layers because\n    # unlike those the output layer actually contains learned parameters\n    vocab_size, hidden_size = weights[\"par_dec.out.weight\"].shape\n    network.par_dec.out = nn.Linear(hidden_size, vocab_size).to(config.DEV)\n    # actually load the parameters\n    network.load_state_dict(weights, strict=False)\n\n    loss_fn = nn.CrossEntropyLoss()\n\n    val_batch_iter = utils.pair_iter(pairs=val_pairs, word2index=word2index, shuffle=False, shuffle_pairs=False)\n\n    total_val_loss = 0\n    if bleu == \"sacrebleu\":\n        total_val_bleu = 0\n    elif bleu == \"nltk\":\n        total_val_bleu_1 = 0\n        total_val_bleu_2 = 0\n        total_val_bleu_3 = 0\n        total_val_bleu_4 = 0\n\n    network.eval()\n    for i, batch in enumerate(val_batch_iter, 1):\n        input, target = batch\n        prediction, words, attentions, fixations = network(input, target)\n\n        loss = loss_fn(\n            torch.stack(prediction).squeeze(1)[: target[0].shape[0], :], target[0]\n        )\n        total_val_loss += loss.item()\n\n        _prediction = [index2word[_x] for _x in torch.argmax(torch.stack(prediction).squeeze(1)[: target[0].shape[0], :], -1).tolist()]\n        _target = [index2word[_x] for _x in target[0].tolist()]\n        if bleu == \"sacrebleu\":\n            bleu_score = sacrebleu.sentence_bleu(\" \".join(_prediction), \" \".join(_target)).score\n            total_val_bleu += bleu_score\n        elif bleu == \"nltk\":\n            bleu_1_score = utils.bleu(target[0].tolist(), torch.argmax(torch.stack(prediction).squeeze(1)[: target[0].shape[0], :], -1).tolist(), n=1)\n            bleu_2_score = utils.bleu(target[0].tolist(), torch.argmax(torch.stack(prediction).squeeze(1)[: target[0].shape[0], :], -1).tolist(), n=2)\n            bleu_3_score = utils.bleu(target[0].tolist(), torch.argmax(torch.stack(prediction).squeeze(1)[: target[0].shape[0], :], -1).tolist(), n=3)\n            bleu_4_score = utils.bleu(target[0].tolist(), torch.argmax(torch.stack(prediction).squeeze(1)[: target[0].shape[0], :], -1).tolist(), n=4)\n            total_val_bleu_1 += bleu_1_score\n            total_val_bleu_2 += bleu_2_score\n            total_val_bleu_3 += bleu_3_score\n            total_val_bleu_4 += bleu_4_score\n\n        if sentence_statistics:\n            s = [index2word[x] for x in input[0].detach().cpu().tolist()]\n            attentions = attentions.detach().cpu().squeeze().tolist()\n            fixations = fixations.detach().cpu().squeeze().tolist()\n\n            if bleu == \"sacrebleu\":\n                print(f\"{bleu_score}\\t{s}\\t{_prediction}\\t{_target}\\t{attentions}\\t{fixations}\")\n            elif bleu == \"nltk\":\n                print(f\"{bleu_1_score}\\t{bleu_2_score}\\t{bleu_3_score}\\t{bleu_4_score}\\t{s}\\t{_prediction}\\t{_target}\\t{attentions}\\t{fixations}\")\n\n    if bleu == \"sacrebleu\":\n        print(f\"avg_val_loss {total_val_loss/len(val_pairs):.4f} avg_val_bleu {total_val_bleu/len(val_pairs):.2f}\")\n    elif bleu == \"nltk\":\n        print(f\"avg_val_loss {total_val_loss/len(val_pairs):.4f} avg_val_bleu_1 {total_val_bleu_1/len(val_pairs):.2f} avg_val_bleu_2 {total_val_bleu_2/len(val_pairs):.2f} avg_val_bleu_3 {total_val_bleu_3/len(val_pairs):.2f} avg_val_bleu_4 {total_val_bleu_4/len(val_pairs):.2f}\")","metadata":{"_uuid":"185e382b-3dd6-4dc3-a4f5-5789d28bafdb","_cell_guid":"89e3ab11-852e-4572-bd18-ffe45cc8b4ff","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-06T21:11:42.516136Z","iopub.execute_input":"2024-02-06T21:11:42.516497Z","iopub.status.idle":"2024-02-06T21:11:42.540296Z","shell.execute_reply.started":"2024-02-06T21:11:42.516466Z","shell.execute_reply":"2024-02-06T21:11:42.539097Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"def test(corpus_name, model_weights, sentence_statistics, bleu):\n    corpus, word2index, index2word = load_corpus(corpus_name, [\"test\"])\n    test_pairs = corpus[\"test\"]\n\n    if model_weights is not None:\n        logger.info(\"loading model checkpoint\")\n        checkpoint = torch.load(model_weights, map_location=config.DEV)\n        if \"word2index\" in checkpoint:\n            weights = checkpoint[\"weights\"]\n            word2index = checkpoint[\"word2index\"]\n            index2word = {i: w for w, i in word2index.items()}\n        else:\n            asdf\n\n    network = init_network(word2index)\n\n    if model_weights is not None:\n        # remove the embedding layer before loading\n        weights = {k: v for k, v in weights.items() if not \"embedding\" in k}\n        # make a new output layer to match the weights from the checkpoint\n        # we cannot remove it like we did with the embedding layers because\n        # unlike those the output layer actually contains learned parameters\n        vocab_size, hidden_size = weights[\"par_dec.out.weight\"].shape\n        network.par_dec.out = nn.Linear(hidden_size, vocab_size).to(config.DEV)\n        # actually load the parameters\n        network.load_state_dict(weights, strict=False)\n\n    loss_fn = nn.CrossEntropyLoss()\n\n    test_batch_iter = utils.pair_iter(pairs=test_pairs, word2index=word2index, shuffle=False, shuffle_pairs=False)\n\n    total_test_loss = 0\n    if bleu == \"sacrebleu\":\n        total_test_bleu = 0\n    elif bleu == \"nltk\":\n        total_test_bleu_1 = 0\n        total_test_bleu_2 = 0\n        total_test_bleu_3 = 0\n        total_test_bleu_4 = 0\n\n    network.eval()\n    for i, batch in enumerate(test_batch_iter, 1):\n        input, target = batch\n\n        prediction, words, attentions, fixations = network(input, target)\n\n        loss = loss_fn(\n            torch.stack(prediction).squeeze(1)[: target[0].shape[0], :], target[0]\n        )\n        total_test_loss += loss.item()\n\n        _prediction = [index2word[_x] for _x in torch.argmax(torch.stack(prediction).squeeze(1)[: target[0].shape[0], :], -1).tolist()]\n        _target = [index2word[_x] for _x in target[0].tolist()]\n        if bleu == \"sacrebleu\":\n            bleu_score = sacrebleu.sentence_bleu(\" \".join(_prediction), \" \".join( _target)).score\n            total_test_bleu += bleu_score\n        elif bleu == \"nltk\":\n            bleu_1_score = utils.bleu(target[0].tolist(), torch.argmax(torch.stack(prediction).squeeze(1)[: target[0].shape[0], :], -1).tolist(), n=1)\n            bleu_2_score = utils.bleu(target[0].tolist(), torch.argmax(torch.stack(prediction).squeeze(1)[: target[0].shape[0], :], -1).tolist(), n=2)\n            bleu_3_score = utils.bleu(target[0].tolist(), torch.argmax(torch.stack(prediction).squeeze(1)[: target[0].shape[0], :], -1).tolist(), n=3)\n            bleu_4_score = utils.bleu(target[0].tolist(), torch.argmax(torch.stack(prediction).squeeze(1)[: target[0].shape[0], :], -1).tolist(), n=4)\n            total_test_bleu_1 += bleu_1_score\n            total_test_bleu_2 += bleu_2_score\n            total_test_bleu_3 += bleu_3_score\n            total_test_bleu_4 += bleu_4_score\n\n        if sentence_statistics:\n            s = [index2word[x] for x in input[0].detach().cpu().tolist()]\n            attentions = attentions.detach().cpu().squeeze().tolist()\n            fixations = fixations.detach().cpu().squeeze().tolist()\n\n            if bleu == \"sacrebleu\":\n                print(f\"{bleu_score}\\t{s}\\t{_prediction}\\t{attentions}\\t{fixations}\")\n            elif bleu == \"nltk\":\n                print(f\"{bleu_1_score}\\t{bleu_2_score}\\t{bleu_3_score}\\t{bleu_4_score}\\t{s}\\t{_prediction}\\t{attentions}\\t{fixations}\")\n\n    if bleu == \"sacrebleu\":\n        print(f\"avg_test_loss {total_test_loss/len(test_pairs):.4f} avg_test_bleu {total_test_bleu/len(test_pairs):.2f}\")\n    elif bleu == \"nltk\":\n        print(f\"avg_test_loss {total_test_loss/len(test_pairs):.4f} avg_test_bleu_1 {total_test_bleu_1/len(test_pairs):.2f} avg_test_bleu_2 {total_test_bleu_2/len(test_pairs):.2f} avg_test_bleu_3 {total_test_bleu_3/len(test_pairs):.2f} avg_test_bleu_4 {total_test_bleu_4/len(test_pairs):.2f}\")","metadata":{"_uuid":"bd609b37-6eac-4cae-8e51-c2f47a2f5a0f","_cell_guid":"2faf85a3-2380-4294-814a-19a14eb4e503","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-06T21:11:46.347837Z","iopub.execute_input":"2024-02-06T21:11:46.348179Z","iopub.status.idle":"2024-02-06T21:11:46.370697Z","shell.execute_reply.started":"2024-02-06T21:11:46.348151Z","shell.execute_reply":"2024-02-06T21:11:46.369752Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"def predict(corpus_name, model_weights):\n    corpus, word2index, index2word = load_corpus(corpus_name, [\"val\"])\n    test_pairs = corpus[\"val\"]\n    s_list = []\n    fixations_list = []\n\n    if model_weights is not None:\n        logger.info(\"loading model checkpoint\")\n        checkpoint = torch.load(model_weights, map_location=config.DEV)\n        if \"word2index\" in checkpoint:\n            weights = checkpoint[\"weights\"]\n            word2index = checkpoint[\"word2index\"]\n            index2word = {i: w for w, i in word2index.items()}\n        else:\n            asdf\n\n    network = init_network(word2index)\n\n    logger.info(f\"vocab size {len(word2index)}\")\n\n    if model_weights is not None:\n        # remove the embedding layer before loading\n        weights = {k: v for k, v in weights.items() if \"embedding\" not in k}\n        # make a new output layer to match the weights from the checkpoint\n        # we cannot remove it like we did with the embedding layers because\n        # unlike those the output layer actually contains learned parameters\n        vocab_size, hidden_size = weights[\"par_dec.out.weight\"].shape\n        network.par_dec.out = nn.Linear(hidden_size, vocab_size).to(config.DEV)\n        # actually load the parameters\n        network.load_state_dict(weights, strict=False)\n\n    test_batch_iter = utils.pair_iter(pairs=test_pairs, word2index=word2index, shuffle=False, shuffle_pairs=False)\n\n    network.eval()\n    for i, batch in enumerate(test_batch_iter, 1):\n        if i > 5:\n            break  # exit the loop after 5 iterations\n\n        input, target, rating = batch\n\n        prediction, words, attentions, fixations = network(input, target)\n        _prediction = [index2word[_x] for _x in torch.argmax(torch.stack(prediction).squeeze(1), -1).tolist()]\n\n        s = [index2word[x] for x in input[0].detach().cpu().tolist()]\n        attentions = attentions.detach().cpu().squeeze().tolist()\n        fixations = fixations.detach().cpu().squeeze().tolist()\n\n        s_list.append(s)\n        fixations_list.append(fixations)\n\n    return s_list, fixations_list\n\n# Example usage:\ns_list, fixations_list = predict(\"qqp\", \"/kaggle/input/supporting-files/my_model_12.tar\")\n","metadata":{"execution":{"iopub.status.busy":"2024-02-06T19:59:24.621950Z","iopub.execute_input":"2024-02-06T19:59:24.622400Z","iopub.status.idle":"2024-02-06T20:03:46.247629Z","shell.execute_reply.started":"2024-02-06T19:59:24.622367Z","shell.execute_reply":"2024-02-06T20:03:46.246646Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"128 4\n#parameters: 89921670\n","output_type":"stream"}]},{"cell_type":"code","source":"def predict1(corpus_name, model_weights):\n    corpus, word2index, index2word = load_corpus(corpus_name, [\"val\"])\n    test_pairs = corpus[\"val\"]\n\n    if model_weights is not None:\n        logger.info(\"loading model checkpoint\")\n        checkpoint = torch.load(model_weights, map_location=config.DEV)\n        if \"word2index\" in checkpoint:\n            weights = checkpoint[\"weights\"]\n            word2index = checkpoint[\"word2index\"]\n            index2word = {i: w for w, i in word2index.items()}\n        else:\n            asdf\n\n    network = init_network(word2index)\n\n    logger.info(f\"vocab size {len(word2index)}\")\n    \n    if model_weights is not None:\n        # remove the embedding layer before loading\n        weights = {k: v for k, v in weights.items() if not \"embedding\" in k}\n        # make a new output layer to match the weights from the checkpoint\n        # we cannot remove it like we did with the embedding layers because\n        # unlike those the output layer actually contains learned parameters\n        vocab_size, hidden_size = weights[\"par_dec.out.weight\"].shape\n        network.par_dec.out = nn.Linear(hidden_size, vocab_size).to(config.DEV)\n        # actually load the parameters\n        network.load_state_dict(weights, strict=False)\n\n    test_batch_iter = utils.pair_iter(pairs=test_pairs, word2index=word2index, shuffle=False, shuffle_pairs=False)\n\n    network.eval()\n    for i, batch in enumerate(test_batch_iter, 1):\n        if i>5:\n            break\n        input, target, rating = batch\n\n        prediction, words, attentions, fixations = network(input, target)\n        _prediction = [index2word[_x] for _x in torch.argmax(torch.stack(prediction).squeeze(1), -1).tolist()]\n\n        s = [index2word[x] for x in input[0].detach().cpu().tolist()]\n        attentions = attentions.detach().cpu().squeeze().tolist()\n        fixations = fixations.detach().cpu().squeeze().tolist()\n\n        print(f\"{s}\\t{_prediction}\\t{attentions}\\t{fixations}\")","metadata":{"execution":{"iopub.status.busy":"2024-02-06T21:11:57.493595Z","iopub.execute_input":"2024-02-06T21:11:57.494360Z","iopub.status.idle":"2024-02-06T21:11:57.506161Z","shell.execute_reply.started":"2024-02-06T21:11:57.494326Z","shell.execute_reply":"2024-02-06T21:11:57.505235Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"import os\nos.remove(\"/kaggle/working/glove.cache\")","metadata":{"execution":{"iopub.status.busy":"2024-01-27T17:29:53.384093Z","iopub.execute_input":"2024-01-27T17:29:53.384467Z","iopub.status.idle":"2024-01-27T17:29:53.416765Z","shell.execute_reply.started":"2024-01-27T17:29:53.384437Z","shell.execute_reply":"2024-01-27T17:29:53.415459Z"},"trusted":true},"execution_count":18,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremove\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/kaggle/working/glove.cache\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/working/glove.cache'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/kaggle/working/glove.cache'","output_type":"error"}]},{"cell_type":"code","source":"def re_train(corpus_name, model_name, bleu, fixation_weights=None, freeze_fixations=False):\n    corpus, word2index, index2word = load_corpus(corpus_name, [\"train\", \"val\"])\n    train_pairs = corpus[\"train\"]\n    val_pairs = corpus[\"val\"]\n    network = init_network(word2index)\n\n    model_dir = os.path.join(\"models\", model_name)\n    logger.debug(\"creating model dir %s\" % model_dir)\n    pathlib.Path(model_dir).mkdir(parents=True)\n\n    if fixation_weights is not None:\n        logger.info(\"loading fixation prediction checkpoint\")\n        checkpoint = torch.load(fixation_weights, map_location=config.DEV)\n        if \"word2index\" in checkpoint:\n            weights = checkpoint[\"weights\"]\n        else:\n            weights = checkpoint\n\n        # remove the embedding layer before loading\n        weights = {k: v for k, v in weights.items() if not k.startswith(\"pre.embedding_layer\")}\n        network.fix_gen.load_state_dict(weights, strict=False)\n\n        if freeze_fixations:\n            logger.info(\"freezing fixation generation network\")\n            for p in network.fix_gen.parameters():\n                p.requires_grad = False\n\n    loss_fn = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(network.parameters(), lr=config.learning_rate)\n    #optimizer = torch.optim.Adam(network.parameters(), lr=1e-4, weight_decay=1e-5)\n\n    epoch = 3\n    # Load previously saved model and word2index\n    checkpoint_path = '/kaggle/input/supporting-files/my_model_3.tar'\n    checkpoint = torch.load(checkpoint_path, map_location=config.DEV)\n    network.load_state_dict(checkpoint[\"weights\"])\n\n    word2index = checkpoint[\"word2index\"]\n    best_val_loss = 5.471\n    \n    while epoch<8:\n        train_batch_iter = utils.pair_iter(pairs=train_pairs, word2index=word2index, shuffle=True, shuffle_pairs=False)\n        val_batch_iter = utils.pair_iter(pairs=val_pairs, word2index=word2index, shuffle=False, shuffle_pairs=False)\n        # test_batch_iter = utils.pair_iter(pairs=test_pairs, word2index=word2index, shuffle=False, shuffle_pairs=False)\n\n        running_train_loss = 0\n        total_train_loss = 0\n        total_val_loss = 0\n\n        if bleu == \"sacrebleu\":\n            running_train_bleu = 0\n            total_train_bleu = 0\n            total_val_bleu = 0\n        elif bleu == \"nltk\":\n            running_train_bleu_1 = 0\n            running_train_bleu_2 = 0\n            running_train_bleu_3 = 0\n            running_train_bleu_4 = 0\n            total_train_bleu_1 = 0\n            total_train_bleu_2 = 0\n            total_train_bleu_3 = 0\n            total_train_bleu_4 = 0\n            total_val_bleu_1 = 0\n            total_val_bleu_2 = 0\n            total_val_bleu_3 = 0\n            total_val_bleu_4 = 0\n\n        network.train()\n        for i, batch in enumerate(train_batch_iter, 1):\n            optimizer.zero_grad()\n\n            input, target,rating = batch\n            prediction, words, attention, fixations = network(input, target)\n\n            loss = loss_fn(\n                torch.stack(prediction).squeeze(1)[: target[0].shape[0], :], target[0]\n            )\n\n            loss.backward()\n            optimizer.step()\n\n            running_train_loss += loss.item()\n            total_train_loss += loss.item()\n\n            _prediction = \" \".join([index2word[_x] for _x in torch.argmax(torch.stack(prediction).squeeze(1)[: target[0].shape[0], :], -1).tolist()])\n            _target = [index2word[_x] for _x in target[0].tolist()]\n\n            if bleu == \"sacrebleu\":\n                bleu_score = sacrebleu.sentence_bleu(_prediction, _target).score\n                running_train_bleu += bleu_score\n                total_train_bleu += bleu_score\n            elif bleu == \"nltk\":\n                bleu_1_score = utils.bleu(target[0].tolist(), torch.argmax(torch.stack(prediction).squeeze(1)[: target[0].shape[0], :], -1).tolist(), n=1)\n                bleu_2_score = utils.bleu(target[0].tolist(), torch.argmax(torch.stack(prediction).squeeze(1)[: target[0].shape[0], :], -1).tolist(), n=2)\n                bleu_3_score = utils.bleu(target[0].tolist(), torch.argmax(torch.stack(prediction).squeeze(1)[: target[0].shape[0], :], -1).tolist(), n=3)\n                bleu_4_score = utils.bleu(target[0].tolist(), torch.argmax(torch.stack(prediction).squeeze(1)[: target[0].shape[0], :], -1).tolist(), n=4)\n                running_train_bleu_1 += bleu_1_score\n                running_train_bleu_2 += bleu_2_score\n                running_train_bleu_3 += bleu_3_score\n                running_train_bleu_4 += bleu_4_score\n                total_train_bleu_1 += bleu_1_score\n                total_train_bleu_2 += bleu_2_score\n                total_train_bleu_3 += bleu_3_score\n                total_train_bleu_4 += bleu_4_score\n            # print(target[0].tolist(), torch.argmax(torch.stack(prediction).squeeze(1)[: target[0].shape[0], :], -1).tolist())\n\n            if i % 1000 == 0:\n\n                if bleu == \"sacrebleu\":\n\n                    print(f\"step {i} avg_train_loss {running_train_loss/100:.4f} avg_train_bleu {running_train_bleu/100:.2f}\")\n                elif bleu == \"nltk\":\n                    print(f\"step {i} avg_train_loss {running_train_loss/100:.4f} avg_train_bleu_1 {running_train_bleu_1/100:.2f} avg_train_bleu_2 {running_train_bleu_2/100:.2f} avg_train_bleu_3 {running_train_bleu_3/100:.2f} avg_train_bleu_4 {running_train_bleu_4/100:.2f}\")\n\n                network.eval()\n                with open(os.path.join(model_dir, f\"debug_{epoch}_{i}.out\"), \"w\") as h:\n                    if bleu == \"sacrebleu\":\n                        h.write(f\"# avg_train_loss {running_train_loss/100:.4f} avg_train_bleu {running_train_bleu/100:.2f}\")\n                        running_train_bleu = 0\n                    elif bleu == \"nltk\":\n                        h.write(f\"# avg_train_loss {running_train_loss/100:.4f} avg_train_bleu_1 {running_train_bleu_1/100:.2f} avg_train_bleu_2 {running_train_bleu_2/100:.2f} avg_train_bleu_3 {running_train_bleu_3/100:.2f} avg_train_bleu_4 {running_train_bleu_4/100:.2f}\")\n                        running_train_bleu_1 = 0\n                        running_train_bleu_2 = 0\n                        running_train_bleu_3 = 0\n                        running_train_bleu_4 = 0\n\n                    running_train_loss = 0\n\n                    h.write(\"\\n\")\n                    h.write(\"\\t\".join([\"sentence\", \"prediction\", \"attention\", \"fixations\"]))\n                    h.write(\"\\n\")\n                    for s, input in zip(debug_sentences, utils.sent_iter(debug_sentences, word2index=word2index)):\n                        prediction, words, attentions, fixations = network(input)\n                        prediction = torch.argmax(torch.stack(prediction).squeeze(1), -1).detach().cpu().tolist()\n                        prediction = [index2word.get(x, \"<__UNK__>\") for x in prediction]\n                        attentions = attentions.detach().cpu().squeeze().tolist()\n                        fixations = fixations.detach().cpu().squeeze().tolist()\n                        h.write(f\"{s}\\t{prediction}\\t{attentions}\\t{fixations}\")\n                        h.write(\"\\n\")\n\n                network.train()\n\n        network.eval()\n        for i, batch in enumerate(val_batch_iter):\n            input, target ,rating = batch\n            prediction, words, attention, fixations = network(input, target, teacher_forcing_ratio=0)\n            loss = loss_fn(\n                torch.stack(prediction).squeeze(1)[: target[0].shape[0], :], target[0]\n            )\n\n            _prediction = \" \".join([index2word[_x] for _x in torch.argmax(torch.stack(prediction).squeeze(1)[: target[0].shape[0], :], -1).tolist()])\n            _target = [index2word[_x] for _x in target[0].tolist()]\n            if bleu == \"sacrebleu\":\n                bleu_score = sacrebleu.sentence_bleu(_prediction, _target).score\n                total_val_bleu += bleu_score\n            elif bleu == \"nltk\":\n                bleu_1_score = utils.bleu(target[0].tolist(), torch.argmax(torch.stack(prediction).squeeze(1)[: target[0].shape[0], :], -1).tolist(), n=1)\n                bleu_2_score = utils.bleu(target[0].tolist(), torch.argmax(torch.stack(prediction).squeeze(1)[: target[0].shape[0], :], -1).tolist(), n=2)\n                bleu_3_score = utils.bleu(target[0].tolist(), torch.argmax(torch.stack(prediction).squeeze(1)[: target[0].shape[0], :], -1).tolist(), n=3)\n                bleu_4_score = utils.bleu(target[0].tolist(), torch.argmax(torch.stack(prediction).squeeze(1)[: target[0].shape[0], :], -1).tolist(), n=4)\n                total_val_bleu_1 += bleu_1_score\n                total_val_bleu_2 += bleu_2_score\n                total_val_bleu_3 += bleu_3_score\n                total_val_bleu_4 += bleu_4_score\n\n            total_val_loss += loss.item()\n\n        avg_val_loss = total_val_loss/len(val_pairs)\n\n        if bleu == \"sacrebleu\":\n            print(f\"epoch {epoch} avg_train_loss {total_train_loss/len(train_pairs):.4f} avg_val_loss {avg_val_loss:.4f} avg_train_bleu {total_train_bleu/len(train_pairs):.2f} avg_val_bleu {total_val_bleu/len(val_pairs):.2f}\")\n        elif bleu == \"nltk\":\n            print(f\"epoch {epoch} avg_train_loss {total_train_loss/len(train_pairs):.4f} avg_val_loss {avg_val_loss:.4f} avg_train_bleu_1 {total_train_bleu_1/len(train_pairs):.2f} avg_train_bleu_2 {total_train_bleu_2/len(train_pairs):.2f} avg_train_bleu_3 {total_train_bleu_3/len(train_pairs):.2f} avg_train_bleu_4 {total_train_bleu_4/len(train_pairs):.2f} avg_val_bleu_1 {total_val_bleu_1/len(val_pairs):.2f} avg_val_bleu_2 {total_val_bleu_2/len(val_pairs):.2f} avg_val_bleu_3 {total_val_bleu_3/len(val_pairs):.2f} avg_val_bleu_4 {total_val_bleu_4/len(val_pairs):.2f}\")\n\n        with open(os.path.join(model_dir, f\"debug_{epoch}_end.out\"), \"w\") as h:\n            if bleu == \"sacrebleu\":\n                h.write(f\"# avg_train_loss {total_train_loss/len(train_pairs)} avg_val_loss {total_val_loss/len(val_pairs)} avg_train_bleu {total_train_bleu/len(train_pairs)} avg_val_bleu {total_val_bleu/len(val_pairs)}\")\n            elif bleu == \"nltk\":\n                h.write(f\"# avg_train_loss {total_train_loss/len(train_pairs):.4f} avg_val_loss {avg_val_loss:.4f} avg_train_bleu_1 {total_train_bleu_1/len(train_pairs):.2f} avg_train_bleu_2 {total_train_bleu_2/len(train_pairs):.2f} avg_train_bleu_3 {total_train_bleu_3/len(train_pairs):.2f} avg_train_bleu_4 {total_train_bleu_4/len(train_pairs):.2f} avg_val_bleu_1 {total_val_bleu_1/len(val_pairs):.2f} avg_val_bleu_2 {total_val_bleu_2/len(val_pairs):.2f} avg_val_bleu_3 {total_val_bleu_3/len(val_pairs):.2f} avg_val_bleu_4 {total_val_bleu_4/len(val_pairs):.2f}\")\n            h.write(\"\\n\")\n            h.write(\"\\t\".join([\"sentence\", \"prediction\", \"attention\", \"fixations\"]))\n            h.write(\"\\n\")\n            for s, input in zip(debug_sentences, utils.sent_iter(debug_sentences, word2index=word2index)):\n                prediction, words, attentions, fixations = network(input)\n                prediction = torch.argmax(torch.stack(prediction).squeeze(1), -1).detach().cpu().tolist()\n                prediction = [index2word.get(x, \"<__UNK__>\") for x in prediction]\n                attentions = attentions.detach().cpu().squeeze().tolist()\n                fixations = fixations.detach().cpu().squeeze().tolist()\n                h.write(f\"{s}\\t{prediction}\\t{attentions}\\t{fixations}\")\n                h.write(\"\\n\")\n                \n        utils.save_model(network, word2index, os.path.join(model_dir, f\"{model_name}_{epoch}\"))\n\n        if best_val_loss is None or avg_val_loss < best_val_loss:\n            if best_val_loss is not None:\n                logger.info(f\"{avg_val_loss} < {best_val_loss} ({avg_val_loss-best_val_loss}): new best model from epoch {epoch}\")\n            else:\n                logger.info(f\"{avg_val_loss} < {best_val_loss}: new best model from epoch {epoch}\")\n\n            best_val_loss = avg_val_loss\n            # save_model(model, word2index, model_name + \"_epoch_\" + str(epoch))\n            # utils.save_model(network, word2index, os.path.join(model_dir, f\"{model_name}_best\"))\n\n        epoch += 1","metadata":{"execution":{"iopub.status.busy":"2024-01-27T17:28:49.912008Z","iopub.execute_input":"2024-01-27T17:28:49.912392Z","iopub.status.idle":"2024-01-27T17:28:49.962104Z","shell.execute_reply.started":"2024-01-27T17:28:49.912359Z","shell.execute_reply":"2024-01-27T17:28:49.960934Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"import os\nos.chdir(r'/kaggle/working')","metadata":{"execution":{"iopub.status.busy":"2024-01-27T16:21:05.194220Z","iopub.execute_input":"2024-01-27T16:21:05.194582Z","iopub.status.idle":"2024-01-27T16:21:05.198938Z","shell.execute_reply.started":"2024-01-27T16:21:05.194554Z","shell.execute_reply":"2024-01-27T16:21:05.198039Z"},"trusted":true},"execution_count":2,"outputs":[]}]}